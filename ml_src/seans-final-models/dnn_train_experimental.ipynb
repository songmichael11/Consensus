{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of Final DNN\n",
    "This jupyter notebook file is where the final version of the DNN is trained and saved. It is written to be fully reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTraceback (most recent call last):\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "\u001b[1;31m    return _run_code(code, main_globals, None,\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "\u001b[1;31m    exec(code, run_globals)\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "\u001b[1;31m    app.launch_new_instance()\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n",
      "\u001b[1;31m    app.initialize(argv)\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/site-packages/traitlets/config/application.py\", line 118, in inner\n",
      "\u001b[1;31m    return method(app, *args, **kwargs)\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 707, in initialize\n",
      "\u001b[1;31m    self.init_kernel()\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 555, in init_kernel\n",
      "\u001b[1;31m    kernel = kernel_factory(\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/site-packages/traitlets/config/configurable.py\", line 583, in instance\n",
      "\u001b[1;31m    inst = cls(*args, **kwargs)\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 154, in __init__\n",
      "\u001b[1;31m    import appnope  # type:ignore[import-untyped]\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/site-packages/appnope/__init__.py\", line 15, in <module>\n",
      "\u001b[1;31m    from ._nope import *  # noqa\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/site-packages/appnope/_nope.py\", line 9, in <module>\n",
      "\u001b[1;31m    import ctypes\n",
      "\u001b[1;31m  File \"/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/ctypes/__init__.py\", line 8, in <module>\n",
      "\u001b[1;31m    from _ctypes import Union, Structure, Array\n",
      "\u001b[1;31mImportError: dlopen(/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/lib-dynload/_ctypes.cpython-310-darwin.so, 0x0002): Library not loaded: @rpath/libffi.7.dylib\n",
      "\u001b[1;31m  Referenced from: <38159387-87A6-38F4-A6A2-1945477CAB3F> /opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/lib-dynload/_ctypes.cpython-310-darwin.so\n",
      "\u001b[1;31m  Reason: tried: '/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/lib-dynload/../../libffi.7.dylib' (no such file), '/opt/anaconda3/envs/temp-doc-tfff/lib/python3.10/lib-dynload/../../libffi.7.dylib' (no such file), '/opt/anaconda3/envs/temp-doc-tfff/bin/../lib/libffi.7.dylib' (no such file), '/opt/anaconda3/envs/temp-doc-tfff/bin/../lib/libffi.7.dylib' (no such file), '/usr/local/lib/libffi.7.dylib' (no such file), '/usr/lib/libffi.7.dylib' (no such file, not in dyld cache). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('../datasets/MEGAFRAME_CLEANEDV2.csv')\n",
    "X = df.drop(columns=['UNEMP', 'Reference area', 'REF_AREA', 'TIME_PERIOD'])\n",
    "y = df['UNEMP']\n",
    "\n",
    "categorical_features = ['Region']\n",
    "numerical_features = X.columns.difference(categorical_features)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "])\n",
    "\n",
    "# Create proper train/validation/test split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.15, random_state=RANDOM_SEED, stratify=None\n",
    ")\n",
    "X_train_full, X_val, y_train_full, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.15, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Data split:\")\n",
    "print(f\"  Training: {len(X_train_full)} samples ({len(X_train_full)/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(X_val)} samples ({len(X_val)/len(X_processed)*100:.1f}%)\")  \n",
    "print(f\"  Test (unseen): {len(X_test)} samples ({len(X_test)/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"  Total: {len(X_processed)} samples\\n\")\n",
    "\n",
    "def create_model(architecture, dropout_rate, l2_reg, learning_rate, input_shape):\n",
    "    \"\"\"Create model with specified hyperparameters\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer\n",
    "    model.add(Dense(architecture[0], activation='relu', \n",
    "                   kernel_regularizer=l2(l2_reg), input_shape=(input_shape,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in architecture[1:]:\n",
    "        model.add(Dense(units, activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "        model.add(BatchNormalization())\n",
    "        if units > 16:  # Only add dropout to larger layers\n",
    "            model.add(Dropout(dropout_rate * 0.5))  # Reduce dropout in deeper layers\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_hyperparameters(params, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Evaluate hyperparameters using train/validation split\"\"\"\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(\n",
    "        architecture=params['architecture'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        l2_reg=params['l2_reg'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        input_shape=X_train.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7, verbose=0)\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=200,\n",
    "        batch_size=params['batch_size'],\n",
    "        callbacks=[early_stop, lr_scheduler],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate with multiple metrics\n",
    "    val_predictions = model.predict(X_val, verbose=0)\n",
    "    train_predictions = model.predict(X_train, verbose=0)\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    val_mse = mean_squared_error(y_val, val_predictions)\n",
    "    val_mae = mean_absolute_error(y_val, val_predictions)\n",
    "    val_r2 = r2_score(y_val, val_predictions)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, train_predictions)\n",
    "    overfitting_gap = val_mse - train_mse\n",
    "    \n",
    "    return {\n",
    "        'val_mse': val_mse,\n",
    "        'val_mae': val_mae,\n",
    "        'val_r2': val_r2,\n",
    "        'overfitting_gap': overfitting_gap,\n",
    "        'params': params\n",
    "    }\n",
    "\n",
    "def detect_overfitting(history):\n",
    "    \"\"\"Analyze training history for overfitting signs\"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    # Find minimum validation loss epoch\n",
    "    min_val_epoch = np.argmin(val_loss)\n",
    "    min_val_loss = val_loss[min_val_epoch]\n",
    "    train_loss_at_min_val = train_loss[min_val_epoch]\n",
    "    \n",
    "    # Calculate overfitting metrics\n",
    "    overfitting_gap = min_val_loss - train_loss_at_min_val\n",
    "    \n",
    "    # Check if validation loss starts increasing while training loss decreases\n",
    "    recent_epochs = 20\n",
    "    if len(val_loss) > recent_epochs:\n",
    "        recent_val_trend = np.polyfit(range(recent_epochs), val_loss[-recent_epochs:], 1)[0]\n",
    "        recent_train_trend = np.polyfit(range(recent_epochs), train_loss[-recent_epochs:], 1)[0]\n",
    "    else:\n",
    "        recent_val_trend = 0\n",
    "        recent_train_trend = 0\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"OVERFITTING ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Final Training Loss:   {train_loss[-1]:.4f}\")\n",
    "    print(f\"Final Validation Loss: {val_loss[-1]:.4f}\")\n",
    "    print(f\"Overfitting Gap:       {overfitting_gap:.4f}\")\n",
    "    \n",
    "    if overfitting_gap > 0.5:\n",
    "        print(\"âš ï¸  HIGH OVERFITTING DETECTED\")\n",
    "        print(\"   - Validation loss significantly higher than training loss\")\n",
    "        print(\"   - Consider: More regularization, less model complexity, more data\")\n",
    "    elif overfitting_gap > 0.2:\n",
    "        print(\"âš ï¸  MODERATE OVERFITTING\")\n",
    "        print(\"   - Some overfitting present but manageable\")\n",
    "        print(\"   - Consider: Slight increase in regularization\")\n",
    "    else:\n",
    "        print(\"âœ… GOOD GENERALIZATION\")\n",
    "        print(\"   - Training and validation losses are similar\")\n",
    "    \n",
    "    if recent_val_trend > 0 and recent_train_trend < 0:\n",
    "        print(\"âš ï¸  DIVERGING TRENDS DETECTED\")\n",
    "        print(\"   - Validation loss increasing while training loss decreasing\")\n",
    "    \n",
    "    return overfitting_gap\n",
    "\n",
    "# Define hyperparameter search space\n",
    "hyperparameter_space = {\n",
    "    'architecture': [\n",
    "        [128, 64, 32],        # Original smaller\n",
    "        [256, 128, 64],       # Medium\n",
    "        [512, 256, 128, 64],  # Larger\n",
    "        [256, 128, 64, 32, 16] # Deeper\n",
    "    ],\n",
    "    'dropout_rate': [0.05, 0.1, 0.2],\n",
    "    'l2_reg': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01],\n",
    "    'batch_size': [8, 16, 32]\n",
    "}\n",
    "\n",
    "print(\"Starting Hyperparameter Optimization...\")\n",
    "print(f\"Total combinations to test: {len(list(product(*hyperparameter_space.values())))}\")\n",
    "print(\"This will take a while - testing top combinations only...\\n\")\n",
    "\n",
    "# Centered search around winning hyperparameters: [256,128,64], batch=32, dropout=0.1, l2=0.001, lr=0.001\n",
    "results = []\n",
    "\n",
    "# Architecture variations centered around winning [256, 128, 64]\n",
    "winning_architectures = [\n",
    "    [256, 128, 64],          # Original winner\n",
    "    [224, 112, 56],          # Slightly smaller\n",
    "    [288, 144, 72],          # Slightly larger  \n",
    "    [256, 128, 64, 32],      # Add one more layer\n",
    "    [320, 160, 80],          # 25% larger\n",
    "    [384, 192, 96],          # 50% larger\n",
    "    [256, 128],              # Shallower version\n",
    "    [256, 192, 128, 64],     # Different taper\n",
    "]\n",
    "\n",
    "# Batch sizes centered around winning 32\n",
    "batch_sizes = [24, 28, 32, 36, 40]\n",
    "\n",
    "# Dropout rates centered around winning 0.1\n",
    "dropout_rates = [0.08, 0.1, 0.12, 0.15]\n",
    "\n",
    "# L2 regularization centered around winning 0.001\n",
    "l2_values = [0.0005, 0.001, 0.002, 0.003]\n",
    "\n",
    "# Learning rates centered around winning 0.001\n",
    "learning_rates = [0.0008, 0.001, 0.0012, 0.0015]\n",
    "\n",
    "test_combinations = []\n",
    "\n",
    "# Test each architecture with optimal baseline params\n",
    "for arch in winning_architectures:\n",
    "    test_combinations.append({\n",
    "        'architecture': arch,\n",
    "        'dropout_rate': 0.1,\n",
    "        'l2_reg': 0.001,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32\n",
    "    })\n",
    "\n",
    "# Test batch size variations with winning architecture\n",
    "for bs in batch_sizes:\n",
    "    if bs != 32:  # Don't duplicate the baseline\n",
    "        test_combinations.append({\n",
    "            'architecture': [256, 128, 64],\n",
    "            'dropout_rate': 0.1,\n",
    "            'l2_reg': 0.001,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': bs\n",
    "        })\n",
    "\n",
    "# Test dropout variations with winning architecture  \n",
    "for dropout in dropout_rates:\n",
    "    if dropout != 0.1:  # Don't duplicate the baseline\n",
    "        test_combinations.append({\n",
    "            'architecture': [256, 128, 64],\n",
    "            'dropout_rate': dropout,\n",
    "            'l2_reg': 0.001,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32\n",
    "        })\n",
    "\n",
    "# Test L2 regularization variations\n",
    "for l2 in l2_values:\n",
    "    if l2 != 0.001:  # Don't duplicate the baseline\n",
    "        test_combinations.append({\n",
    "            'architecture': [256, 128, 64],\n",
    "            'dropout_rate': 0.1,\n",
    "            'l2_reg': l2,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32\n",
    "        })\n",
    "\n",
    "# Test learning rate fine-tuning\n",
    "for lr in learning_rates:\n",
    "    if lr != 0.001:  # Don't duplicate the baseline\n",
    "        test_combinations.append({\n",
    "            'architecture': [256, 128, 64],\n",
    "            'dropout_rate': 0.1,\n",
    "            'l2_reg': 0.001,\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': 32\n",
    "        })\n",
    "\n",
    "# Add a few promising combinations based on earlier results\n",
    "test_combinations.extend([\n",
    "    # Larger model with more regularization\n",
    "    {'architecture': [384, 192, 96], 'dropout_rate': 0.12, 'l2_reg': 0.002, 'learning_rate': 0.001, 'batch_size': 32},\n",
    "    # Deeper model with careful regularization\n",
    "    {'architecture': [256, 192, 128, 64], 'dropout_rate': 0.1, 'l2_reg': 0.0015, 'learning_rate': 0.001, 'batch_size': 32},\n",
    "    # Best combo with slightly different batch size\n",
    "    {'architecture': [256, 128, 64], 'dropout_rate': 0.1, 'l2_reg': 0.001, 'learning_rate': 0.001, 'batch_size': 28},\n",
    "])\n",
    "\n",
    "for i, params in enumerate(test_combinations):\n",
    "    print(f\"Testing combination {i+1}/{len(test_combinations)}: {params}\")\n",
    "    result = evaluate_hyperparameters(params, X_train_full, y_train_full, X_val, y_val)\n",
    "    results.append(result)\n",
    "    print(f\"  Val MSE: {result['val_mse']:.4f} | Val MAE: {result['val_mae']:.2f} | RÂ²: {result['val_r2']:.3f} | Overfitting: {result['overfitting_gap']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Find best parameters (prioritize MSE but consider overfitting)\n",
    "best_result = min(results, key=lambda x: x['val_mse'])\n",
    "best_balanced = min([r for r in results if r['overfitting_gap'] < 4.0], \n",
    "                   key=lambda x: x['val_mse'], default=best_result)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"HYPERPARAMETER OPTIMIZATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\nAll Results (sorted by validation score):\")\n",
    "for i, result in enumerate(sorted(results, key=lambda x: x['val_score'])):\n",
    "    print(f\"{i+1}. Val Score: {result['val_score']:.4f} | \"\n",
    "          f\"Overfitting: {result['overfitting_gap']:.4f} | {result['params']}\")\n",
    "\n",
    "print(f\"\\nðŸ† BEST PARAMETERS:\")\n",
    "print(f\"   Architecture: {best_result['params']['architecture']}\")\n",
    "print(f\"   Dropout Rate: {best_result['params']['dropout_rate']}\")\n",
    "print(f\"   L2 Regularization: {best_result['params']['l2_reg']}\")\n",
    "print(f\"   Learning Rate: {best_result['params']['learning_rate']}\")\n",
    "print(f\"   Batch Size: {best_result['params']['batch_size']}\")\n",
    "print(f\"   Validation Score: {best_result['val_score']:.4f}\")\n",
    "print(f\"   Overfitting Gap: {best_result['overfitting_gap']:.4f}\")\n",
    "\n",
    "# Train final model with best parameters and analyze overfitting\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "final_model = create_model(\n",
    "    architecture=best_result['params']['architecture'],\n",
    "    dropout_rate=best_result['params']['dropout_rate'],\n",
    "    l2_reg=best_result['params']['l2_reg'],\n",
    "    learning_rate=best_result['params']['learning_rate'],\n",
    "    input_shape=X_train_full.shape[1]\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7, verbose=1)\n",
    "\n",
    "history = final_model.fit(\n",
    "    X_train_full, y_train_full,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=500,\n",
    "    batch_size=best_result['params']['batch_size'],\n",
    "    callbacks=[early_stop, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Analyze overfitting\n",
    "overfitting_gap = detect_overfitting(history)\n",
    "\n",
    "# FINAL TEST EVALUATION (completely unseen data)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL TEST EVALUATION (COMPLETELY UNSEEN DATA)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "test_loss = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "test_predictions = final_model.predict(X_test, verbose=0)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"Test Results (Never seen this data before!):\")\n",
    "print(f\"  Test Loss (MSE): {test_loss[0]:.4f}\")\n",
    "print(f\"  Test MAE: {test_mae:.2f}\")\n",
    "print(f\"  Test RÂ² Score: {test_r2:.3f}\")\n",
    "\n",
    "# Compare with validation performance\n",
    "val_loss = min(history.history['val_loss'])\n",
    "print(f\"\\nValidation vs Test Comparison:\")\n",
    "print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"  Test Loss:       {test_loss[0]:.4f}\")\n",
    "print(f\"  Difference:      {abs(test_loss[0] - val_loss):.4f}\")\n",
    "\n",
    "if abs(test_loss[0] - val_loss) < 0.1:\n",
    "    print(\"âœ… EXCELLENT: Test and validation performance are very similar!\")\n",
    "    print(\"   Your model generalizes well to completely unseen data.\")\n",
    "elif abs(test_loss[0] - val_loss) < 0.3:\n",
    "    print(\"âœ… GOOD: Test and validation performance are reasonably similar.\")\n",
    "    print(\"   Your model generalizes adequately.\")\n",
    "else:\n",
    "    print(\"âš ï¸  WARNING: Large difference between test and validation performance.\")\n",
    "    print(\"   Your hyperparameter tuning may have overfit to the validation set.\")\n",
    "\n",
    "# Save the optimized model\n",
    "final_model.save('Unemployment_AI_Optimized.keras')\n",
    "\n",
    "print(f\"\\nâœ… Optimized model saved as 'Unemployment_AI_Optimized.keras'\")\n",
    "print(f\"   Final test performance: MAE = {test_mae:.2f}, RÂ² = {test_r2:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp-doc-tfff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
